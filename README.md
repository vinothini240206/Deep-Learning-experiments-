# Deep-Learning-experiments-
This repository features experiments with MINST and CIFAR-10 datasets, including:  Logistic regression for cat recognition Single-layer neural network Deep neural networks Various initialization methods Regularization techniques Optimization algorithms Convolution and pooling in NumPy TensorFlow ConvNet RNN in NumPy Character-level language model.
This repository contains a series of experiments conducted using the MINST and CIFAR-10 datasets. The primary focus is on building and optimizing various machine learning models, ranging from logistic regression classifiers to advanced neural networks. Below is a detailed list of the components included in this repository:

Components
Build Logistic Regression Classifier to Recognize Cats:

Implemented a logistic regression model to classify images of cats.
Implement Single Layer Neural Network for Binary Classification:

Developed a single-layer neural network to perform binary classification tasks.
Building Deep Neural Network for Image Classification:

Constructed a deep neural network to classify images from the CIFAR-10 dataset.
Training Deep Neural Network with Various Ways of Initialization:

Experimented with different weight initialization methods to train deep neural networks and analyzed their impacts on model performance.
Apply L2 Regularisation and Dropout Regularisation to Improve Accuracy of Model:

Applied L2 regularization and dropout techniques to prevent overfitting and improve the accuracy of the models.
Optimization of Deep Neural Network using Mini Batch Gradient Descent and Adam Optimization:

Optimized deep neural networks using mini-batch gradient descent and the Adam optimization algorithm.
Implement Convolution and Pooling Layers using Numpy:

Implemented convolution and pooling layers from scratch using NumPy to understand the underlying mechanisms.
Implement ConvNet using TensorFlow:

Built a convolutional neural network (ConvNet) using TensorFlow for image classification.
Building RNN in Numpy:

Developed a recurrent neural network (RNN) from scratch using NumPy for sequence prediction tasks.
Building Character Level Language Model:

Created a character-level language model to predict the next character in a sequence.
Each experiment includes detailed explanations, code implementations, and results. The aim is to provide a comprehensive understanding of different machine learning and deep learning techniques, their implementation, and their optimization.
